from pyspark.sql import (
    SparkSession,
    types,
    functions as F,
)

# Initialize Spark Session
spark = (
    SparkSession
    .builder
    .appName('cleaning_orders_dataset_with_pyspark')
    .getOrCreate()
)

# Load Data
orders_data = spark.read.parquet('orders_data.parquet')
orders_data.toPandas().head()

# Step 1: Data Cleaning and Preprocessing

# Create a new column for time of day based on order date
orders_data = (
    orders_data
    .withColumn(
        'time_of_day',
        F.when((F.hour('order_date') >= 0) & (F.hour('order_date') <= 5), 'night')
         .when((F.hour('order_date') >= 6) & (F.hour('order_date') <= 11), 'morning')
         .when((F.hour('order_date') >= 12) & (F.hour('order_date') <= 17), 'afternoon')
         .when((F.hour('order_date') >= 18) & (F.hour('order_date') <= 23), 'evening')
         .otherwise(None)
    )
    .filter(F.col('time_of_day') != 'night')
    .withColumn(
        'order_date',
        F.col('order_date').cast(types.DateType())
    )
)

# Step 2: Data Transformation

# Convert product and category columns to lowercase and remove rows containing "tv" in product column
orders_data = (
    orders_data
    .withColumn('product', F.lower('product'))
    .withColumn('category', F.lower('category'))
    .filter(~F.col('product').contains('tv'))
)

# Step 3: Extract and Clean Address Data

# Extract state information from purchase address
orders_data = (
    orders_data
    .withColumn('address_split', F.split('purchase_address', ' '))
    .withColumn('purchase_state', F.col('address_split').getItem(F.size('address_split') - 2))
    .drop('address_split')
)

# Step 4: Calculate Number of Unique States

n_states = (
    orders_data
    .select('purchase_state')
    .distinct()
    .count()
)

# Step 5: Export Cleaned Data

# Export the cleaned data to a new parquet file
(
    orders_data
    .write
    .parquet(
        'orders_data_clean.parquet',
        mode='overwrite',
    )
)
